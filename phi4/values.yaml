# Default values for ollama-stack
# This is a YAML-formatted file.

nameOverride: ""
fullnameOverride: ""

# Domain configuration for ingress
domain: internal-stg-lb.spot.rxt.sys

# Ollama configuration
ollama:
  image:
    repository: ollama/ollama
    tag: latest
    pullPolicy: IfNotPresent
  
  # LLM model to use
  models:
    run:
      - "phi4-mini:3.8b"
  
  resources:
    limits:
      cpu: 16
      memory: 16Gi
      nvidia.com/gpu: 1
    requests:
      cpu: 1
      memory: 2Gi
      nvidia.com/gpu: 1

  service:
    type: ClusterIP
    port: 11434
    targetPort: 11434

# Tiny Ollama UI configuration
tinyOllamaUI:
  image:
    repository:  ghcr.io/roopakparikh/tiny-ollama-chat
    tag: latest
    pullPolicy: Always
  
  resources:
    requests:
      cpu: 1
      memory: 2Gi
  
  service:
    type: ClusterIP
    port: 8080
    targetPort: 8080
  
  env:
    - name: PUBLIC_APP_NAME
      value: "Ollama Web UI"

# Ingress configuration
ingress:
  ingressClass: nginx
  enabled: true
  annotations:
    kubernetes.io/ingress-class: nginx
    nginx.ingress.kubernetes.io/rewrite-target: /$1
    nginx.ingress.kubernetes.io/use-regex: "true"
    nginx.ingress.kubernetes.io/ssl-redirect: "true"
    cert-manager.io/cluster-issuer: selfsigned-issuer
    nginx.ingress.kubernetes.io/proxy-read-timeout: "3600"
    nginx.ingress.kubernetes.io/proxy-send-timeout: "3600"
    nginx.ingress.kubernetes.io/server-snippets: |
      location / {
        proxy_set_header Upgrade $http_upgrade;
        proxy_http_version 1.1;
        proxy_set_header X-Forwarded-Host $http_host;
        proxy_set_header X-Forwarded-Proto $scheme;
        proxy_set_header X-Forwarded-For $remote_addr;
        proxy_set_header Host $host;
        proxy_set_header Connection "upgrade";
        proxy_cache_bypass $http_upgrade;
      }
  tls:
    enabled: true
